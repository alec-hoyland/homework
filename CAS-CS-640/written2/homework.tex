\documentclass[11pt,largemargins, anonymous]{homework}

\usepackage[titlesize=18pt]{colophon}

% TODO: replace these with your information
\newcommand{\hwname}{Alec Hoyland}
\newcommand{\hwemail}{U83403624}
\newcommand{\hwtype}{Homework}
\newcommand{\hwnum}{1}
\newcommand{\hwclass}{CAS CS 640}
\newcommand{\hwlecture}{0}
\newcommand{\hwsection}{A1/A5}

\begin{document}

\clearpage
\maketitle
\clearpage

\question

\begin{alphaparts}
    \questionpart
    $ \mathrm{accuracy} = (\mathrm{TP + TN}) / (\mathrm{P + N}) = (16 + 8) / (20 + 10) = 24/30 = 0.8$.

    \questionpart
    The point in ROC space is at $\left ( FP / (FP + TN) , TP / (TP + TN) \right )$.
    Therefore, $ ( 4 / (4 + 8) , 16 / (16 + 8) ) = (0.33, 0.67)$.

    \questionpart
    The F-1 score is $2 TP / (2 TP + FP + FN)$.
    Therefore, $F_1 = 2 * 16 / (2 * 16 + 4 + 2) = 0.84$
\end{alphaparts}

\question
In this neural network, the activation function is an x-shifted Heaviside function.

\begin{alphaparts}
    \questionpart
    The input vector is $[0, 0, 1, 0, 0, 1]$.
    This leads to a hidden layer state of $[2, 2]$, which is activated to
    $[1, 1]$.
    This leads to an output layer state of $[4, -4]$, which is activated to $[1, 0]$.
    Thus, the neural network concludes that Romeo and Juliet are acquaintances.

    \questionpart
    All the weights are going to be multiplied by a constant $C$.
    Since the activation functions are linear and the other operations are inner products
    (which are linear), the neural network mapping from input to output is itself a linear mapping.
    Since homogeneity is a property of all linear systems, for any linear mapping $f$,
    $f(Cx) = C f(x)$.
    This tells us that the output vector as a function of the input vector is $\vec{y} = f(\vec{x})$,
    which tells us that for $f(C \vec{x})$, the resultant vector $C \vec{y}$, when normalized,
    results in the same probabalistic interpretation as the unscaled neural network.

    Here is an example, with the same input vector as before.

    The input vector is $[0, 0, 1, 0, 0, 1]$.
    This leads to a hidden layer state of $[2C, 2C]$, which is activated to
    $[1, 1]$.
    This leads to an output layer state of $[4C, -4C]$, which is activated to $[1, 0]$.
    Thus, the neural network concludes that Romeo and Juliet are acquaintances.

\end{alphaparts}

\question

\begin{alphaparts}
    \questionpart

    \[ \frac{\partial P}{\partial o_z} = \frac{1 - d_z}{1 - o_z} - \frac{d_z}{o_z}  \]

    \questionpart

    Since $o_z(w) = \sigma (w \cdot o_h)$ (I am omitting any indices here, so $\cdot$ represents scalar multiplication),

    \[ \frac{\partial o_z}{\partial w} = \sigma^\prime (w \cdot o_h) o_h = \sigma (w \cdot o_h ) (1 - \sigma (w \cdot o_h)) o_h \]

    \questionpart

    \[ \frac{\partial P}{\partial w} = \frac{\partial P}{\partial o_z} \frac{\partial o_z}{\partial w} = \bigg( \frac{1 - d_z}{1 - o_z} - \frac{d_z}{o_z} \bigg) \bigg( \sigma (w \cdot o_h ) (1 - \sigma (w \cdot o_h)) o_h \bigg) \]

    \questionpart

    \[ w_{new} = w - r \cdot \frac{\partial P}{\partial w} \]

    \questionpart

    \begin{align*}
        w_{new} &= 0.4 - 0.1 \cdot \bigg( \frac{1 - 0.9}{1 - 0.7} - \frac{0.9}{0.7} \bigg) \bigg( \sigma (0.4 \cdot 0.8 )  \Big( 1 - \sigma (0.4 \cdot 0.8 ) \Big) 0.8 \bigg) \\
        w_{new} &= 0.4 - 0.1 \cdot \bigg( \frac{0.1}{0.3} - 1.3 \bigg) \bigg( 0.58 (1 - 0.58) 0.8 \bigg) \\
        w_{new} &= 0.4 - 0.1 \cdot -0.97 \cdot 0.195 \\
        w_{new} &= 0.42 \\
    \end{align*}

\end{alphaparts}

\begin{colophon}
    This document was typeset using the \LaTeXe{} document processing system
    originally developed by Leslie Lamport, based on the \TeX{} typesetting system
    created by Donald Knuth.
    The class is \texttt{latex-homework-class} by Jake Zimmerman,
    released under the MIT license.
    All above work was done by the authors.
    I used Bishop's machine learning textbook as reference.
\end{colophon}

\end{document}
