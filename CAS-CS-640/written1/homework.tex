\documentclass[11pt,largemargins, anonymous]{homework}

\usepackage[titlesize=18pt]{colophon}

% TODO: replace these with your information
\newcommand{\hwname}{Alec Hoyland}
\newcommand{\hwemail}{U83403624}
\newcommand{\hwtype}{Homework}
\newcommand{\hwnum}{1}
\newcommand{\hwclass}{CAS CS 640}
\newcommand{\hwlecture}{0}
\newcommand{\hwsection}{A1/A5}

\begin{document}
\maketitle

\part{Responsible AI}

\question

\textbf{Who codes matters}

Code is written by people with thoughts on how to implement a certain algorithm or tool.
What their experiences are directly influences what sorts of bugs or flaws we notice,
and what our priorities are.
Joy's facial recognition problems in college were caused by engineers
neglecting to test their software on people with dark complexions.
\\

\textbf{How we code matters}

Rigorous testing should include not just bug testing, but construct validation.
Is your algorithm biased? In statistics, this can be rigorously evaluated,
but if equity in your dataset isn't on the forefront of your mind,
it's easy to omit construct validation testing if no one makes it a point to remember.
\\

\textbf{Why we code matters}

The purpose, or objective, of code shapes how it affects the world.
If code is written, someone is often footing the bill, someone with their own motivations.
Code written for the EFF has a much different use-case than that written for police departments.
We should keep the teleology of our software in mind.

\question

\begin{arabicparts}

    \questionpart

I ducked for ``Northpointe AI''. Here are the first four relevant results:

\begin{enumerate}
    \item "Rise of the racist robots" (The Guardian)
    \item "Inspecting algorithms for bias" (MIT Tech Review)
    \item "Towards a code of ethics in artificial intelligence" (Future of Life)
    \item "Machine Bias" (ProPublica)
\end{enumerate}

    Let me begin by saying, \textit{holy shit}, this is \textit{not} a responsible use of AI.
    There are obvious benefits to using AI in the legal system, for the same reason that it's useful in medicine.
    Seeing a judge or doctor is a time-intensive endeavor, since there are not a lot of judges and doctors,
    and reviewing a case reviews synthesizing a lot of documentation.
    Handling large amounts of data and making a decision is a thing that artificial intelligence is great at.
    The problem is, neural networks just do a kind of nonlinear regression
    where the basis functions and the weights vary during training.
    This means that neural networks are the victims of their training datasets,
    and the twin curses of overfitting and dimensionality.

    In the ProPublica report, they showed that not only were the Northpointe ROC curves shadier than previously reported,
    but that when compared race-by-score, that black defendants were actually less at-risk than white defendants for recidivism.
    In order to test for false positives or false negatives, they culled the dataset,
    and found that in all cases, black defendants were judged too harshly by the algorithm.

    I believe this is likely due to a flawed dataset.
    Reality does not lie, but data are what they measure, not what you think they measure.
    If the legal system of Broward County included even an iota of racism, \textit{e.g.}
    harsher sentencing, more arrests/criminal complaints, fewer economic and educational opportunities...
    this would likely show up in the dataset used to train the \texttt{COMPAS} algorithm.
    If racism shows up in the dataset, then it will show up in the trained classifier.

    While I understand that automated classification will speed up the legal process -- which is a good thing,
    it needs to be rigorously tested for bias before being deployed,
    since people's lives and freedoms are at risk here.
    Broward County and the \texttt{COMPAS} designers should be held responsible.

    \questionpart

    Deferral rates refer to cases where the algorithm does not make a decision,
    but passes the case to a human evaluator instead.
    At this point, there are two possible options.
    Either the case is evaluated, or it is tabled for later.
    In the case of a prisoner, this is the difference between being considered for parole,
    and being left in prison until the next possible parole date,
    which is tantamount to a "high risk" score by the \texttt{COMPAS} algorithm.

    So long as the first case is true, it will result in a generally more fair legal system,
    provided that the people running the legal apparatus are not racists themselves.
    The deferral rate essentially represents how much work cannot be offloaded to the algorithm,
    so an algorithm with a deferral rate of about 20\% with respect to the total caseload,
    will require humans to do 20\% of the work themselves.
    So, an algorithm with a low deferral rate is better for the workloads of the legal workers.
    Of course, one could design an algorithm that just flips a coin for each case,
    and has a deferral rate of 0\%.
    That would be the most efficient, and save so much paperwork!
    It also has the advantage of not being racist at all.
    But of course, there are other issues to consider.

    If the deferral rate is uneven between races or genders or what-have you,
    this basically results in a system where one group is being punished more for the same crimes,
    because they are serving longer sentences.
    This has economic and social ramifications,
    since the incarcerated are essentially disenfranchised in political, social, and economic spheres.

    \questionpart

    Canetti \textit{et al.} discuss several different methods for making AI decision systems more equitable.
    In Example 3.1, they show that even using a group-blind post-processor,
    the unprivileged group is held to a higher standard in order to make the positive predictive values equal.
    This means that a non-deferring threshold post-processor has fundamental
    difficulties promoting totally equitable situations.
    The deferral rates are higher in the Canetti model vs. the original \texttt{COMPAS} algorithm,
    because Canetti \textit{et al.} are taking the extra step of equalizing the accuracies
    by deferring more cases.
    This leads to an increased number of deferrals.
    While deferring only on the smaller group does result in fewer total referrals (without loss of equity),
    there are still more deferrals than a naive approach (\textit{viz.} without post-processing).

    \questionpart

    The authors aren't sure why this happens.
    In the two-threshold method, it's clear that when the classifier is unsure
    (\textit{i.e.} the classifier score is middling) that the classifier defers.
    I suspect this result happens because of the minimization procedure.
    Removing extremely high- or low-scoring cases would swing the distribution,
    since there are fewer cases scoring in the extrema.
    I would guess that the minimization algorithm is picking up on this,
    and this is causing the "shoe-in" cases to be deferred.

\end{arabicparts}

\part{Rule-based Systems}

\question

Use a forward-chaining algorithm to determine what the book is named.
The set of assertions in the working memory are:

    A1: Book X has two copies \\
    A2: Book X is missing pages \\
    A3: Book X has tea stains \\
    A4: Book X is about science \\
    A5: Book X is donated by Prof. Betke \\
    A6: Book X is heavy \\

\begin{tabular}{lll}
    \textbf{Rule matched} & \textbf{With assertion(s)} & \textbf{Assertion addition(s)} \\
    R7 & A5, A6 & A8: Book X is heavy \\
    R11 & A8, A1, A2, A3, A4 & A9: Book X is \textit{AI}
\end{tabular}

\question

Use a backward-chaining algorithm to prove that the book is \textit{Artificial Intelligence}
using a depth-first search order.

The set of assertions in the working memory are:

BCA1: Book X is \textit{Artificial Intelligence}.

\begin{tabular}{lll}
    \textbf{Rule matched} & \textbf{With assertion(s)} & \textbf{Assertion addition(s)} \\
    R11 & BCA1 & BCA2: Book X is textbook \\
    & & BCA3: Book X has two copies \\
    & & BCA4: Book X is missing pages \\
    & & BCA5: Book X has tea stains \\
    & & BCA6: Book X is about science \\
\end{tabular}

Now that we have exhausted the only rule where the consequence is
"is \textit{Artificial Intelligence}",
we consider rules one at a time,
where the consequence is a "backward-chaining" assertion.

\begin{tabular}{lll}
    \textbf{Rule matched} & \textbf{With assertion(s)} & \textbf{Assertion addition(s)} \\
    R6 & BCA2 & BCA7: Book X is donated by Prof. Betke \\
    & & BCA8: Book X has notes
\end{tabular}

We can't prove BCA8, so we backtrack.

\begin{tabular}{lll}
    \textbf{Rule matched} & \textbf{With assertion(s)} & \textbf{Assertion addition(s)} \\
    R7 & BCA2 & BCA7: Book X is donated by Prof. Betke \\
    & & BCA9: Book X is heavy
\end{tabular}

We now have confirmed all of our working memory assertions via back-chaining.

\question

A1: Book X has two copies \\
A2: Book X is missing pages \\
A3: Book X has tea stains \\
A4: Book X is about science \\
A5: Book X is donated by Prof. Betke \\
A6: Book X is heavy \\
A7: Book X has doodles

\begin{arabicparts}

\questionpart

\begin{tabular}{lll}
    \textbf{Rule matched} & \textbf{With assertion(s)} & \textbf{Assertion addition(s)} \\
    R5 & A5, A7 & A8: Book X is fiction \\
    R7 & A5, A6 & A9: Book X is textbook \\
    R9 & A8, A3, A4 & Book X is \textit{I, Robot}
\end{tabular}

This is a different answer than before, because of the precedence of the rules.
I followed the rules in order, $1-n$.
When a rule was shown to be true, given the assertions,
I added the consequence to the working memory assertion list,
and started over at Rule 1, skipping any rules that were already proven.

\questionpart

\begin{enumerate}

    \item With backward-chaining, we begin with R11,
    and show that A1, A2, A3, and A4 are satisfied.
    We need to confirm that "A8: Book X is textbook" is true though.

    \item R6 and R7 have the consequence, "Book X is textbook".
    We begin with R6, and find that A5 is satisfied.
    We must prove now that "A9: Book X has notes" is true though.

    \item We cannot prove if Book X has notes. There is no rule for that.
    So we default to trying to prove R7.

    \item R7 is used when A5 and A7 are satisified. A5 and A7 are in our working memory,
    so we are finished.

\end{enumerate}

Backward-chaining says that the book is \textit{Artificial Intelligence}.

\end{arabicparts}

\question

We cannot add to, or remove from the assertion list,
and we cannot add to or remove any the rules we initially had.
We can do the following:
(a) add new rules, and
(b) reorder rules.

The problem is that the rule that leads to \textit{I, Robot}
has a set of claims that are a subset of the claims that lead to \textit{Artificial Intelligence}.

By moving the precedence of R7 up to above R5, we could solve the problem,
at least in this particular instance.
Then, we would conclude that the book is \textit{AI} first,
without reaching the conclusion that the book could be fiction.

We could also add a rule that precludes a book from being both a textbook and fiction.
We would need two rules: $\mathrm{Fiction}(X) \Rightarrow \neg \mathrm{Textbook}(X)$ and $\mathrm{Textbook}(X) \Rightarrow \neg \mathrm{Fiction}(X)$. These rules would have to have high precedence,
and wouldn't protect us if we were using a depth-first search,
since we would conclude that the book is \textit{I, Robot} if we didn't restart at the top of the rule list.
This might make the problem undecidable though,
since we would get to a contradiction,
and then have to restart entirely.

\part{Logic and Planning}

\question

Convert block-world assertions into axioms in 1st-order logic
without any free variables.

\begin{arabicparts}

    \questionpart \( \mathrm{On}(b, a) \)
    \questionpart \( \mathrm{On}(A, table) \)
    \questionpart \( \mathrm{On}(c, table) \lor \mathrm{On}(c, a) \)
    \questionpart \( \mathrm{BlueBlock}(d) \land \mathrm{On}(d, table) \)
    \questionpart \( \exists z [\mathrm{RedBlock}(z)] \land \exists w [\mathrm{BlueBlock}(w)] \)
    \questionpart \( \forall x, y [(\mathrm{On}(x, y) \land \mathrm{Equal}(y, table)) \Rightarrow \mathrm{On}(x, table)] \)
    \questionpart \( \forall x, y [\mathrm{Pyramid}(x) \Rightarrow  \neg \mathrm{On}(y, x)] \)
    \questionpart \( \forall x, y [\mathrm{Pyramid}(x) \Rightarrow  \neg \mathrm{On}(y, x)] \)

\end{arabicparts}

\part{ROC Analysis}

% Your content

\begin{colophon}
    This document was typeset using the \LaTeXe{} document processing system
    originally developed by Leslie Lamport, based on the \TeX{} typesetting system
    created by Donald Knuth.
    The class is \texttt{latex-homework-class} by Jake Zimmerman,
    released under the MIT license.
    All above work was done by the authors.
\end{colophon}

\end{document}
