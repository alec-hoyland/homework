
@article{tsaoIntegratingTimeExperience2018,
  title = {Integrating Time from Experience in the Lateral Entorhinal Cortex},
  volume = {561},
  copyright = {2018 Springer Nature Limited},
  issn = {1476-4687},
  abstract = {Temporal information that is useful for episodic memory is encoded across a wide range of timescales in the lateral entorhinal cortex, arising inherently from its representation of ongoing experience.},
  language = {en},
  number = {7721},
  journal = {Nature},
  doi = {10.1038/s41586-018-0459-6},
  author = {Tsao, Albert and Sugar, J{\o}rgen and Lu, Li and Wang, Cheng and Knierim, James J. and Moser, May-Britt and Moser, Edvard I.},
  month = sep,
  year = {2018},
  pages = {57-62},
  file = {/home/alec/Zotero/storage/8DKY236A/Tsao et al. - 2018 - Integrating time from experience in the lateral en.pdf;/home/alec/Zotero/storage/9JDLURS9/s41586-018-0459-6.html}
}

@article{shankarScaleinvariantInternalRepresentation2012,
  title = {A Scale-Invariant Internal Representation of Time},
  volume = {24},
  issn = {1530-888X},
  abstract = {We propose a principled way to construct an internal representation of the temporal stimulus history leading up to the present moment. A set of leaky integrators performs a Laplace transform on the stimulus function, and a linear operator approximates the inversion of the Laplace transform. The result is a representation of stimulus history that retains information about the temporal sequence of stimuli. This procedure naturally represents more recent stimuli more accurately than less recent stimuli; the decrement in accuracy is precisely scale invariant. This procedure also yields time cells that fire at specific latencies following the stimulus with a scale-invariant temporal spread. Combined with a simple associative memory, this representation gives rise to a moment-to-moment prediction that is also scale invariant in time. We propose that this scale-invariant representation of temporal stimulus history could serve as an underlying representation accessible to higher-level behavioral and cognitive mechanisms. In order to illustrate the potential utility of this scale-invariant representation in a variety of fields, we sketch applications using minimal performance functions to problems in classical conditioning, interval timing, scale-invariant learning in autoshaping, and the persistence of the recency effect in episodic memory across timescales.},
  language = {eng},
  number = {1},
  journal = {Neural Computation},
  doi = {10.1162/NECO_a_00212},
  author = {Shankar, Karthik H. and Howard, Marc W.},
  month = jan,
  year = {2012},
  keywords = {Animals,Models; Neurological,Neurons,Neural Networks (Computer),Humans,Algorithms,Learning,Computational Biology,Time Factors,Memory,Conditioning; Classical,Memory; Episodic},
  pages = {134-193},
  pmid = {21919782}
}

@article{tiganjEncodingLaplaceTransform2013,
  title = {Encoding the {{Laplace}} Transform of Stimulus History Using Mechanisms for Persistent Firing},
  volume = {14},
  issn = {1471-2202},
  number = {Suppl 1},
  journal = {BMC Neuroscience},
  doi = {10.1186/1471-2202-14-S1-P356},
  author = {Tiganj, Zoran and Shankar, Karthik H and Howard, Marc W},
  month = jul,
  year = {2013},
  pages = {P356},
  file = {/home/alec/Zotero/storage/3V7KWDMV/Tiganj et al. - 2013 - Encoding the Laplace transform of stimulus history.pdf},
  pmid = {null},
  pmcid = {PMC3704764}
}

@book{hilleFunctionalAnalysisSemigroups1957,
  series = {American {{Mathematical Society Colloquium Publications}}, Vol. 31},
  title = {Functional Analysis and Semi-Groups},
  publisher = {{American Mathematical Society, Providence, R. I.}},
  author = {Hille, Einar and Phillips, Ralph S.},
  year = {1957},
  file = {/home/alec/Zotero/storage/NA8XSCLR/mathscinet-getitem.html},
  mrnumber = {0089373}
}

@article{webbFactorisedNeuralRelational2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.08721},
  primaryClass = {cs, stat},
  title = {Factorised {{Neural Relational Inference}} for {{Multi}}-{{Interaction Systems}}},
  abstract = {Many complex natural and cultural phenomena are well modelled by systems of simple interactions between particles. A number of architectures have been developed to articulate this kind of structure, both implicitly and explicitly. We consider an unsupervised explicit model, the NRI model, and make a series of representational adaptations and physically motivated changes. Most notably we factorise the inferred latent interaction graph into a multiplex graph, allowing each layer to encode for a different interaction-type. This fNRI model is smaller in size and significantly outperforms the original in both edge and trajectory prediction, establishing a new state-of-the-art. We also present a simplified variant of our model, which demonstrates the NRI's formulation as a variational auto-encoder is not necessary for good performance, and make an adaptation to the NRI's training routine, significantly improving its ability to model complex physical dynamical systems.},
  journal = {arXiv:1905.08721 [cs, stat]},
  author = {Webb, Ezra and Day, Ben and {Andres-Terre}, Helena and Li{\'o}, Pietro},
  month = may,
  year = {2019},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/alec/Zotero/storage/KHIQ95UL/Webb et al. - 2019 - Factorised Neural Relational Inference for Multi-I.pdf;/home/alec/Zotero/storage/I4SMBKBX/1905.html}
}

@article{debrangesStoneWeierstrassTheorem1959,
  title = {The {{Stone}}-{{Weierstrass}} Theorem},
  volume = {10},
  issn = {0002-9939, 1088-6826},
  abstract = {Advancing research. Creating connections.},
  language = {en},
  number = {5},
  journal = {Proceedings of the American Mathematical Society},
  doi = {10.1090/S0002-9939-1959-0113131-7},
  author = {{de Branges}, Louis},
  year = {1959},
  pages = {822-824},
  file = {/home/alec/Zotero/storage/WEALHJSW/de Branges - 1959 - The Stone-Weierstrass theorem.pdf;/home/alec/Zotero/storage/HKS6PS78/home.html}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  volume = {2},
  issn = {1435-568X},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  language = {en},
  number = {4},
  journal = {Mathematics of Control, Signals and Systems},
  doi = {10.1007/BF02551274},
  author = {Cybenko, G.},
  month = dec,
  year = {1989},
  keywords = {Approximation,Completeness,Neural networks},
  pages = {303-314}
}

@incollection{luExpressivePowerNeural2017,
  title = {The {{Expressive Power}} of {{Neural Networks}}: {{A View}} from the {{Width}}},
  shorttitle = {The {{Expressive Power}} of {{Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  publisher = {{Curran Associates, Inc.}},
  author = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {6231--6239},
  file = {/home/alec/Zotero/storage/2SH9P2SX/Lu et al. - 2017 - The Expressive Power of Neural Networks A View fr.pdf;/home/alec/Zotero/storage/9YLS9ST7/7203-the-expressive-power-of-neural-networks-a-view-from-the-width.html}
}

@article{haninApproximatingContinuousFunctions2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.11278},
  primaryClass = {cs, math, stat},
  title = {Approximating {{Continuous Functions}} by {{ReLU Nets}} of {{Minimal Width}}},
  abstract = {This article concerns the expressive power of depth in deep feed-forward neural nets with ReLU activations. Specifically, we answer the following question: for a fixed \$d\_\{in\}\textbackslash{}geq 1,\$ what is the minimal width \$w\$ so that neural nets with ReLU activations, input dimension \$d\_\{in\}\$, hidden layer widths at most \$w,\$ and arbitrary depth can approximate any continuous, real-valued function of \$d\_\{in\}\$ variables arbitrarily well? It turns out that this minimal width is exactly equal to \$d\_\{in\}+1.\$ That is, if all the hidden layer widths are bounded by \$d\_\{in\}\$, then even in the infinite depth limit, ReLU nets can only express a very limited class of functions, and, on the other hand, any continuous function on the \$d\_\{in\}\$-dimensional unit cube can be approximated to arbitrary precision by ReLU nets in which all hidden layers have width exactly \$d\_\{in\}+1.\$ Our construction in fact shows that any continuous function \$f:[0,1]\^\{d\_\{in\}\}\textbackslash{}to\textbackslash{}mathbb R\^\{d\_\{out\}\}\$ can be approximated by a net of width \$d\_\{in\}+d\_\{out\}\$. We obtain quantitative depth estimates for such an approximation in terms of the modulus of continuity of \$f\$.},
  journal = {arXiv:1710.11278 [cs, math, stat]},
  author = {Hanin, Boris and Sellke, Mark},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Computational Complexity,Computer Science - Machine Learning,Mathematics - Combinatorics,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/alec/Zotero/storage/VWQYK5MA/Hanin and Sellke - 2018 - Approximating Continuous Functions by ReLU Nets of.pdf;/home/alec/Zotero/storage/4WM4JC7M/1710.html}
}


