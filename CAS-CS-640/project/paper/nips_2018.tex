\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

\usepackage[preprint]{nips_2018}
% \usepackage[final]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\bibliographystyle{unsrt}

\title{Tooling around with MNIST: what's the best model architecture?}

\author{%
  Alec J.~Hoyland \\
  Center for Systems Neuroscience\\
  Boston University\\
  Boston, MA 02215 \\
  \texttt{ahoyland@bu.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
  This is the abstract.
\end{abstract}

\section{Introduction}

Optical character recognition (OCR) is the ability to take as input an image including handwritten or printed text,
and output the sequence of characters contained within the image.
OCR engines have many applications, including data entry (e.g. for checks, passports, invoices),
book-scanning (e.g. for Project Gutenberg), pen computing, and in assistive devices
(e.g. for blind and visually-impaired individuals).
In an OCR engine, text is read from a scanned document or image,
and translate the images into a form manipulable by the computer, such as plain text,
enabling a multitude of other analyses and operations.
As the saying goes, ``You can't grep dead trees!''

A successful model is one which accurately predicts the correct label for the image
(i.e. recognizes the digit), and does so quickly.
A model that fails to accurately label is not useful, as is one that labels accurately,
but too slowly to be useful in real-world applications.

We describe two measures of efficiency,
training time and testing time.
The former is the total runtime of the training phase,
and the latter is the total runtime of the testing phase.
Since the model need only be trained once (or perhaps fine-tuned later),
minimizing runtime on the testing set is more important,
since in a real-world scenario, the ``testing'' set will be monumentally larger,
consisting of any documents or images the model is used to annotate.

Intuition indicates that simpler models,
which involve less computationally-involved operations,
will perform better on measures of efficiency,
though it is possible that the decrease in accuracy would be unacceptable.

In this paper, we will explore a convolutional neural network and a feedforward neural network,
with various training schedules, and benchmark accuracy and runtime speed.

\section{Related Works}

The MNIST dataset \citep{bottouComparisonClassifierMethods1994, HandwrittenDigitDatabase}
has been used as a standard machine learning benchmark
for more than two decades.
It is comprised of 60,000 28x28 grayscale images of handwritten digits 0-9.
The handwriting samples themselves were written by American high school students,
and American Census Bureau employees.
While concerns about the construct validity of the dataset have been raised in recent years \citep{yadavColdCaseLost2019}
the dataset and its variants remains a staple of machine learning benchmarking,
and serves well for evaluating the quality of various model architectures.

The MNIST dataset is suited to a classification task,
where the image is read as input, and the output is the numerical digit label (i.e. 0-9).
Many different model architectures have been implemented to achieve highest accuracy on the classification task.

Many different model architectures have been tested against the MNIST dataset.
Linear classifiers comprise some of the simplest of models, and traditionally fare poorly on digit identification
\citep{bottouComparisonClassifierMethods1994}.
Some of the first attempts based on multilayer neural networks with backpropagation \citep{bottouComparisonClassifierMethods1994, lecunGradientbasedLearningApplied1998},
were performed by the original curators of the MNIST dataset.
Many variations have been tested, including limited receptive field models \citep{kussulImprovedMethodHandwritten2004},
and convolutional neural networks \citep{simardBestPracticesConvolutional2003}.
The latter exploits elastic training image deformations, which achieved state-of-the-art accuracy.
Image deformation has also been explored in hidden Markov models \citep{keysersDeformationModelsImage2007}.
Since then, methods have improved in GPU-based computations in deep, big simple neural networks
\citep{ciresanDeepBigSimple2010, ciresanMulticolumnDeepNeural2012}.
Support vector machines \citep{decosteTrainingInvariantSupport2002} have also been used for image-recognition tasks.

\section{Methods}

In this paper, four models are implemented and compared to each other.
The goal is to determine what sort of basic model architecture performs the best,
in efficiency of training/evaluating, as well as in accuracy of the resultant classification.

% \subsection{Autoencoder}
% ``
% The autoencoder model encodes the MNIST images as compressed vectors
% that can later be decoded back into images.
% The vector input data is 784-dimensional, and the output dimension of this model is 32-dimensional.
% This implies that the encoding is a strongly compressed (lossy) representation.
%
% The encoder has a single hidden layer with 784 inputs, 32 hidden units, and a leaky
% rectifying linear unit (leaky ReLU) output layer.
%
% The decoder also has a single hidden layer, with 32 input units, and 784 output units.
%
% Training was performed using the mean-squared error, with the Adam optimizer \citep{kingmaAdamMethodStochastic2017}.
% Data were partitioned into batches of size 1000.
%
% Since autoencoding is used for data representation, after training,
% the decoder was scrapped, and replaced with a fully-connected classifier layer with a softargmax output.

\subsection{Convolutional neural network}

The convolutional neural network (CNN) consists of three convolutional layers,
a fully-connected layer, and a softargmax output.

The first convolutional layer operates on a 28x28 image, with a 3x3 window,
padding of (1, 1), and a ReLU activation.
This leads to 16 units in the second convolution layer,
which operate with a 3x3 window and padding of (1, 1) on 14x14 images.
This convolutional layer passes through ReLU activation to a third convolution layer,
which acts on 7x7 images, with the same padding and ReLU activation as before.

Each convolutional layer is separated by a 2x2 maximum pooling layer.

Finally, the 3-dimensional tensor is flattened to 2 dimensions,
and passed to a 288-unit fully-connected layer with linear activation,
and 10 outputs (corresponding to the digits).
The softargmax function is used to squash the outputs to normalized probabilities.

Loss is computed by the cross-entropy function, and the Adam optimizer with $\beta = 0.001$
is used for training \citep{kingmaAdamMethodStochastic2017}.

% Several strategies were implemented to try to improve the model.
% First, the input data was fuzzed with Gaussian noise ($\xi = 0.1$)
% to try to make the model more robust.
% Secondly, early-stopping would be triggered at 99.9\% accuracy,
% in order to prevent overfitting.
% Finally, an adaptive learning rate was implemented.
% If no improvement was seen in 5 epochs, the learning rate was decreased by 90\%
% to a minimum of $10^{-6}$.
% If no improvement was seen after 10 epochs, the model will stop training as well.

\subsection{Fully-connected neural network}

A multi-layer neural network was also tested.
This network consists of 2 fully-connected, feedforward layers each with 32 hidden units.
ReLU activation is used between the hidden layers,
and the softargmax function is used to squash outputs.

The cross-entropy loss is used, and Adam is the optimizer algorithm.

\subsection{Modifications to existing models}

Several alterations were made to the base models to attempt to improve accuracy and efficiency.

First, the input data were fuzzed with Gaussian noise ($\xi = 0.1$)
to try to make the model more robust.
Second, an modified training schedule was implemented,
with early-stopping at 99.9\% accuracy to prevent overfitting,
and an adaptive learning rate.

In the adaptive learning rate schedule,
if no improvement was seen in 5 epochs, the learning rate was decreased by 90\%
to a minimum of $10^{-6}$.
If no improvement was seen after 10 epochs, training stops early,
as the model is assumed to have converged to a local minimum.

These modifications result in a total of eight models evaluated:

\begin{itemize}
  \item \textsc{Conv}, the base convolutional neural network
  \item \textsc{ConvFuzz}, CNN with input fuzzing
  \item \textsc{ConvAdpt}, CNN with adaptive learning rate
  \item \textsc{ConvFull}, CNN with both input fuzzing and adaptive learning rate
  \item \textsc{Dense}, the base fully-connected feedforward neural network
  \item \textsc{DenseFuzz}, feedforward network with input fuzzing
  \item \textsc{DenseAdpt}, feedforward network with adaptive learning rate
  \item \textsc{DenseFull}, feedforward network with both input fuzzing and adaptive learning rate
\end{itemize

\subsection{Evaluating quality of models}

Models were evaluated for accuracy and efficiency.

Accuracy is defined as the number of correct labels divided by the size of the test set.
Efficiency was measured by training time, and testing time.
Training time is defined as the amount of time for which the model is trained.
Testing time is the amount of time it takes for the model to label the testing set.

Unless training time is prohibitively long, testing time is more important.
This is because possessing a model that can identify characters and glyphs quickly,
is important for real-world applications of optical character recognition.
A model does not need only to be accurate, it should also be fast.

% \begin{table}
%   \caption{Models tested}
%   \label{tbl:models}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule(r){1-2}
%     Name     & Description     & Size ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
% \end{table}

\section{Results}

\section{Discussion}

\section{Conclusion}

\bibliography{bibliography}


\end{document}
