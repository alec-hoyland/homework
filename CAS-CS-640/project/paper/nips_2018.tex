\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

\usepackage[preprint]{nips_2018}
% \usepackage[final]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\bibliographystyle{unsrt}

\title{Tooling around with MNIST: what's the best model architecture?}

\author{%
  Alec J.~Hoyland \\
  Center for Systems Neuroscience\\
  Boston University\\
  Boston, MA 02215 \\
  \texttt{ahoyland@bu.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
  This is the abstract.
\end{abstract}

\section{Introduction}

Optical character recognition (OCR) is the ability to take as input an image including handwritten or printed text,
and output the sequence of characters contained within the image.
OCR engines have many applications, including data entry (e.g. for checks, passports, invoices),
book-scanning (e.g. for Project Gutenberg), pen computing, and in assistive devices
(e.g. for blind and visually-impaired individuals).
In an OCR engine, text is read from a scanned document or image,
and translate the images into a form manipulable by the computer, such as plain text,
enabling a multitude of other analyses and operations.
As the saying goes, ``You can't grep dead trees!''

A successful model is one which accurately predicts the correct label for the image
(i.e. recognizes the digit), and does so quickly.
A model that fails to accurately label is not useful, as is one that labels accurately,
but too slowly to be useful in real-world applications.

We describe two measures of efficiency,
training time and testing time.
The former is the total runtime of the training phase,
and the latter is the total runtime of the testing phase.
Since the model need only be trained once (or perhaps fine-tuned later),
minimizing runtime on the testing set is more important,
since in a real-world scenario, the ``testing'' set will be monumentally larger,
consisting of any documents or images the model is used to annotate.

Intuition indicates that simpler models,
which involve less computationally-involved operations,
will perform better on measures of efficiency,
though it is possible that the decrease in accuracy would be unacceptable.

In this paper, we will explore a convolutional neural network and a feedforward neural network,
with various training schedules, and benchmark accuracy and runtime speed.

\section{Related Works}

The MNIST dataset \citep{bottouComparisonClassifierMethods1994, HandwrittenDigitDatabase}
has been used as a standard machine learning benchmark
for more than two decades.
It is comprised of 60,000 28x28 grayscale images of handwritten digits 0-9.
The handwriting samples themselves were written by American high school students,
and American Census Bureau employees.
While concerns about the construct validity of the dataset have been raised in recent years \citep{yadavColdCaseLost2019}
the dataset and its variants remains a staple of machine learning benchmarking,
and serves well for evaluating the quality of various model architectures.

The MNIST dataset is suited to a classification task,
where the image is read as input, and the output is the numerical digit label (i.e. 0-9).
Many different model architectures have been implemented to achieve highest accuracy on the classification task.

Many different model architectures have been tested against the MNIST dataset.
Linear classifiers comprise some of the simplest of models, and traditionally fare poorly on digit identification
\citep{bottouComparisonClassifierMethods1994}.
Some of the first attempts based on multilayer neural networks with backpropagation \citep{bottouComparisonClassifierMethods1994, lecunGradientbasedLearningApplied1998},
were performed by the original curators of the MNIST dataset.
Many variations have been tested, including limited receptive field models \citep{kussulImprovedMethodHandwritten2004},
and convolutional neural networks \citep{simardBestPracticesConvolutional2003}.
The latter exploits elastic training image deformations, which achieved state-of-the-art accuracy.
Image deformation has also been explored in hidden Markov models \citep{keysersDeformationModelsImage2007}.
Since then, methods have improved in GPU-based computations in deep, big simple neural networks
\citep{ciresanDeepBigSimple2010, ciresanMulticolumnDeepNeural2012}.
Support vector machines \citep{decosteTrainingInvariantSupport2002} have also been used for image-recognition tasks.

\section{Methods}

In this paper, eight models are implemented and compared to each other.
The goal is to determine what sort of basic model architecture performs the best,
in efficiency of training/evaluating, as well as in accuracy of the resultant classification.
We consider a convolutional neural network and a fully-connected feedforward neural network,
with modifications to the loss function and training schedule.

\subsection{Convolutional neural network}

The convolutional neural network (CNN) consists of three convolutional layers,
a fully-connected layer, and a softargmax output.

The first convolutional layer operates on a 28x28 image, with a 3x3 window,
padding of (1, 1), and a ReLU activation.
This leads to 16 units in the second convolution layer,
which operate with a 3x3 window and padding of (1, 1) on 14x14 images.
This convolutional layer passes through ReLU activation to a third convolution layer,
which acts on 7x7 images, with the same padding and ReLU activation as before.

Each convolutional layer is separated by a 2x2 maximum pooling layer.

Finally, the 3-dimensional tensor is flattened to 2 dimensions,
and passed to a 288-unit fully-connected layer with linear activation,
and 10 outputs (corresponding to the digits).
The softargmax function is used to squash the outputs to normalized probabilities.

Loss is computed by the cross-entropy function, and the Adam optimizer with $\beta = 0.001$
is used for training \citep{kingmaAdamMethodStochastic2017}.

\subsection{Fully-connected neural network}

A multi-layer neural network was also tested.
This network consists of 2 fully-connected, feedforward layers each with 32 hidden units.
ReLU activation is used between the hidden layers,
and the softargmax function is used to squash outputs.

Cross-entropy loss is used, and Adam is the optimizer algorithm.

\subsection{Modifications to existing models}

Several alterations were made to the base models to attempt to improve accuracy and efficiency.

First, the input data were fuzzed with Gaussian noise ($\xi = 0.1$)
to try to make the model more robust.
Second, a modified training schedule was implemented,
with early-stopping at 99.9\% accuracy to prevent overfitting,
and an adaptive learning rate.

In the adaptive learning rate schedule,
while the loss function was used to train the model,
stopping was determined by the accuracy on the testing dataset.
If no improvement was seen in 5 epochs, the learning rate was decreased by 90\%
to a minimum of $10^{-6}$.
If no improvement was seen after 10 epochs, training stops early,
as the model is assumed to have converged to a local minimum.

These modifications result in a total of eight models:

\begin{itemize}
  \item \textsc{Conv}, the base convolutional neural network
  \item \textsc{ConvFuzz}, CNN with input fuzzing
  \item \textsc{ConvAdpt}, CNN with adaptive learning rate
  \item \textsc{ConvFull}, CNN with both input fuzzing and adaptive learning rate
  \item \textsc{Dense}, the base fully-connected feedforward neural network
  \item \textsc{DenseFuzz}, feedforward network with input fuzzing
  \item \textsc{DenseAdpt}, feedforward network with adaptive learning rate
  \item \textsc{DenseFull}, feedforward network with both input fuzzing and adaptive learning rate
\end{itemize}

\subsection{Evaluating quality of models}

Models were evaluated for accuracy and efficiency.

Accuracy is defined as the number of correct labels divided by the size of the test set.
Efficiency was measured by training time, and testing time.
Training time is defined as the amount of time for which the model is trained.
Testing time is the amount of time it takes for the model to label the testing set.

Unless training time is prohibitively long, testing time is more important.
This is because possessing a model that can identify characters and glyphs quickly,
is important for real-world applications of optical character recognition.
A model does not need only to be accurate, it should also be fast.

\section{Results}

Models were written in \textsc{Julia} \citep{bezansonJuliaFreshApproach2017a}
using the \textsc{Flux} package for autodifferentiation and machine learning \citep{innesFluxElegantMachine2018}.
Computations were performed on a 16-core computer at 3.7 GHz,
with 32 GB of RAM.
No GPU-acceleration was used.

Models were compiled, and timed using built-in macros.

Training time is defined as the amount of time it took to train the model to a stopping point.
In non-adaptive models, this is the time of 5 epochs through the training set (60,000 images).
In adaptive models, the models ran until assumed convergence (10 epochs).

Testing time is defined as the amount of time it took to run the model on the 10,000 image testing set.

Testing accuracy is the ratio of number of correct classifications divided by the total
for all images in the testing set.

We see that the \textsc{Conv}-family of models perform the best in accuracy,
but take the longest to run, both in training time and in testing time.
Convolutional neural networks perform well on image-classification tasks
because they treat the image as a matrix, which can be filtered over several convolutional layers.
In contrast, traditional feedforward \textsc{Dense} networks accept rasterized image input.
The convolutional layers allow sub-features to be extracted out, leading to better classification.
However, convolution is an expensive operation, requiring more time.

We also see that \textsc{Dense} adaptive models perform terribly in accuracy.
The testing accuracy does not decrease fast enough from an untrained (randomly-initialized) model
for the adaptive learning rate heuristic to be of any use.
Instead, the model quits training too early, resulting in a poorly-trained model.

Of the models tested, the adaptive learning rate is useful when training computationally-expensive
models for many epochs.
In these cases, the model will quit when the accuracy is maximized,
avoiding overfitting, or performing excessive training without an increase in accuracy.
Since the number of epochs trained for was low, the adaptive learning rate
did not make a significant difference.

\begin{table}[h]
  \caption{Results of simulations (5 epochs)}
  \label{tbl:results}
  \centering
  \begin{tabular}{llll}
    \toprule
    Model Name & Training Time (s) & Testing Time (s) & Testing Accuracy \\
    \midrule
    \textsc{Conv}         & 320 & 3.72 & 0.985 \\
    \textsc{ConvFuzz}     & 314 & 3.42 & 0.984 \\
    \textsc{ConvAdpt}     & 336 & 3.78 & 0.987 \\
    \textsc{ConvFull}     & 376 & 3.38 & 0.986 \\
    \textsc{Dense}        & 67.9 & 0.053 & 0.906 \\
    \textsc{DenseFuzz}    & 144 & 0.055 & 0.901 \\
    \textsc{DenseAdpt}    & 9.51 & 0.058 & 0.116 \\
    \textsc{DenseFull}    & 0.114 & 0.103 & 0.117 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \caption{Results of simulations (10 epochs)}
  \label{tbl:results2}
  \centering
  \begin{tabular}{llll}
    \toprule
    Model Name & Training Time (s) & Testing Time (s) & Testing Accuracy \\
    \midrule
    \textsc{Conv}         & 762 & 3.83 & 0.986 \\
    \textsc{ConvFuzz}     & 713 & 3.48 & 0.988 \\
    \textsc{ConvAdpt}     & 627 & 3.30 & 0.987 \\
    \textsc{ConvFull}     & 642 & 3.34 & 0.987 \\
    \textsc{Dense}        & 68.3 & 0.062 & 0.900 \\
    \textsc{DenseFuzz}    & 145 & 0.0625 & 0.902 \\
    \textsc{DenseAdpt}    & 22.8 & 0.059 & 0.114 \\
    \textsc{DenseFull}    & 0.114 & 0.103 & 0.117 \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Discussion}

\section{Conclusion}

\bibliography{bibliography}

\end{document}
